# Development-of-Big-Data-Pipeline-for-Healthcare-Analytics-using-Python
Healthcare Analytics Platform

End-to-End Data Pipeline using MongoDB, Apache Airflow, AWS & Streamlit

ğŸ“Œ Project Overview

This project implements a full-stack healthcare analytics system that enables data ingestion, automated ETL, and analytical visualization.

The system consists of:

A web interface for uploading patient and visit data

A batch analytics pipeline orchestrated using Apache Airflow

AWS-based storage and processing

An interactive Streamlit dashboard for insights and monitoring

The goal is to demonstrate production-style data engineering practices including orchestration, cloud storage, automation, and analytics.

ğŸ§± Repository Structure
â”œâ”€â”€ interface-main/        # Frontend interface (Render + Vercel)
â”œâ”€â”€ airflow/               # Airflow DAGs and ETL logic
â”œâ”€â”€ dashboard/             # Streamlit analytics dashboard
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

ğŸ—ï¸ High-Level Architecture
Interface (Vercel / Render)
        â†“
MongoDB (Operational Data Store)
        â†“
Apache Airflow (EC2 â€“ Ubuntu)
        â†“
AWS S3 (Raw + Processed Data)
        â†“
AWS Glue (ETL & Aggregations)
        â†“
Streamlit Dashboard

ğŸ§‘â€ğŸ’» Interface Layer (interface-main/)

Provides Admin and Doctor panels

Used to upload:

Patient data

Visit records

Prescription information

Data is stored in MongoDB

Deployed using:

Render (backend)

Vercel (frontend)

This layer acts as the data producer for the analytics pipeline.

âš™ï¸ Airflow & ETL Pipeline (airflow/)
ğŸ”¹ Infrastructure

Ubuntu Server on AWS EC2

Apache Airflow installed using pip

Airflow runs as a standalone orchestrator

ğŸ”¹ Airflow Responsibilities

Extract data from MongoDB

Upload raw batches to AWS S3

Trigger AWS Glue jobs for:

Data cleaning

Fact table generation

Aggregations

ğŸ”¹ DAG Setup

DAGs are located inside airflow/dags/

DAGs are time-based (scheduled) and manually triggerable

DAG configuration (S3 paths, Glue job names, etc.) is handled directly in code using:

Airflow Variables

Environment-based constants

ğŸ”¹ Airflow Setup (Ubuntu EC2)

High-level steps:

# System setup
sudo apt update
sudo apt install python3-pip -y

# Airflow installation
pip install apache-airflow

# Initialize Airflow
airflow db init
airflow users create
airflow standalone


Runtime files like logs and metadata DB are intentionally excluded from version control.

â˜ï¸ AWS Involvement

AWS is used as the analytics backbone:

Amazon EC2
Hosts Apache Airflow on Ubuntu

Amazon S3

data-raw/ â†’ raw batch uploads from MongoDB

data-processed/ â†’ cleaned & aggregated outputs

AWS Glue

ETL jobs for cleaning healthcare data

Fact table and aggregation generation

Airflow orchestrates AWS services using scheduled DAGs.

ğŸ“Š Analytics Dashboard (dashboard/)
ğŸ”¹ Technology

Built using Streamlit

Reads processed data from AWS S3

Displays:

Visit trends

Patient metrics

Aggregated healthcare insights

ğŸ”¹ Dashboard Setup
pip install -r requirements.txt
streamlit run app.py

ğŸ”¹ AWS Connectivity

Uses AWS credentials via:

IAM Role (recommended on EC2)

or environment variables for local testing

export AWS_ACCESS_KEY_ID=...
export AWS_SECRET_ACCESS_KEY=...
export AWS_REGION=...

ğŸ” Configuration & Security

Sensitive files are excluded via .gitignore

AWS credentials are never committed

Runtime artifacts (logs, cache, DB files) are ignored

ğŸ¯ Key Highlights

End-to-end data engineering workflow

Production-style Airflow orchestration

Cloud-native analytics using AWS

Clean separation of interface, pipeline, and analytics

GitHub-ready, reproducible setup

ğŸš€ Future Enhancements

Real-time ingestion using Kafka/Kinesis

Role-based dashboard access

Integration with real EHR systems
